#!/bin/bash
set -e

echo "--- Setting up Business Collector v2 ---"

# --- 1. Stop and remove the old service to prevent conflicts ---
echo "[+] Stopping and removing old business_collector service..."
sudo systemctl stop business_collector || true
sudo systemctl disable business_collector || true
sudo rm -f /etc/systemd/system/business_collector.service
sudo rm -rf /factory/workers/collectors/business_collector
sudo systemctl daemon-reload

# --- 2. Define Paths ---
PROJECT_DIR="/factory/workers/collectors/business_collector_v2"
LOG_DIR="/factory/logs"
DUMP_DIR="/factory/data/raw"
USER="tdf"

# --- 3. Create Directories ---
echo "[+] Creating project directories..."
mkdir -p $PROJECT_DIR

# --- 4. Create Application Files ---
echo "[+] Creating business_collector_v2.py application file..."
cat << 'EOF' > $PROJECT_DIR/business_collector_v2.py
import os
import requests
import feedparser
import time
import logging
import json
import uuid
import random
from concurrent.futures import ThreadPoolExecutor

# --- Configuration ---
LOG_DIR = "/factory/logs"
RAW_OUTPUT_DIR = "/factory/data/raw"
MAX_WORKERS = 25 # Increased worker count
REST_PERIOD_SECONDS = 60 # 1 minute rest period

SOURCES = {
    'rss': [
        {'url': 'https://feeds.harvard.edu/hbr', 'category': 'business_hbr'},
        {'url': 'https://www.wsj.com/xml/rss/3_7014.xml', 'category': 'finance_wsj'},
        {'url': 'https://www.forbes.com/business/feed/', 'category': 'business_forbes'},
        {'url': 'http://feeds.reuters.com/reuters/businessNews', 'category': 'business_reuters'},
        {'url': 'https://www.economist.com/business/rss.xml', 'category': 'business_economist'},
        {'url': 'https://www.inc.com/rss/index.xml', 'category': 'business_startups'},
        {'url': 'https://www.entrepreneur.com/latest.rss', 'category': 'business_startups'},
        {'url': 'https://feeds.bloomberg.com/markets/rss.xml', 'category': 'finance_bloomberg'}
    ],
    'sec_filings': {
        'ciks': ['0000320193', '0001652044', '0001018724', '0001318605', '0001045810', '0000789019'], # Apple, Alphabet, Amazon, Tesla, Nvidia, Microsoft
        'headers': {'User-Agent': 'Next GenAi Data Factory tdf@example.com'}
    }
}

# --- Setup Logging ---
os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(
    filename=os.path.join(LOG_DIR, 'business_collector_v2.log'),
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logging.getLogger('').addHandler(logging.StreamHandler())

def dump_raw_content(content, source_category, extension):
    try:
        dir_path = os.path.join(RAW_OUTPUT_DIR, source_category)
        os.makedirs(dir_path, exist_ok=True)
        filename = f"business_collector_v2_{int(time.time() * 1000)}_{str(uuid.uuid4())[:8]}.{extension}"
        filepath = os.path.join(dir_path, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        logging.info(f"Dumped file to {filepath}")
    except Exception as e:
        logging.error(f"Failed to dump file for category {source_category}: {e}", exc_info=True)

def scrape_rss_source(source):
    try:
        logging.info(f"Scraping RSS: {source['url']}")
        feed = feedparser.parse(source['url'])
        for entry in feed.entries[:5]:
            try:
                response = requests.get(entry.link, headers={'User-Agent': 'BusinessCollector/2.0'}, timeout=20)
                response.raise_for_status()
                dump_raw_content(response.text, source['category'], 'html')
            except Exception as e:
                logging.warning(f"Could not fetch article {entry.link}: {e}")
    except Exception as e:
        logging.error(f"Could not parse RSS feed {source['url']}: {e}", exc_info=True)

def scrape_sec_filings(sec_config):
    try:
        cik_to_fetch = random.choice(sec_config['ciks'])
        logging.info(f"Fetching 10-K filing for CIK: {cik_to_fetch}")
        api_url = f"https://data.sec.gov/submissions/CIK{cik_to_fetch}.json"
        response = requests.get(api_url, headers=sec_config['headers'], timeout=30)
        response.raise_for_status()
        data = response.json()
        
        filings = data['filings']['recent']
        # Find the most recent 10-K filing
        for i in range(len(filings['form'])):
            if filings['form'][i] == '10-K':
                accession_number = filings['accessionNumber'][i].replace('-', '')
                primary_document = filings['primaryDocument'][i]
                doc_url = f"https://www.sec.gov/Archives/edgar/data/{cik_to_fetch}/{accession_number}/{primary_document}"
                
                logging.info(f"Downloading 10-K from {doc_url}")
                doc_response = requests.get(doc_url, headers=sec_config['headers'], timeout=30)
                doc_response.raise_for_status()
                dump_raw_content(doc_response.text, 'sec_filings', 'html')
                return # Stop after finding the first one
    except Exception as e:
        logging.error(f"Failed to fetch SEC filing: {e}", exc_info=True)

def main():
    while True:
        logging.info("--- Starting new Business Collector v2 cycle ---")
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for source in SOURCES['rss']:
                executor.submit(scrape_rss_source, source)
            # Fetch from two random companies each cycle
            for _ in range(2):
                executor.submit(scrape_sec_filings, SOURCES['sec_filings'])
        logging.info(f"--- Cycle finished. Waiting {REST_PERIOD_SECONDS} seconds... ---")
        time.sleep(REST_PERIOD_SECONDS)

if __name__ == "__main__":
    main()
EOF

cat << 'EOF' > $PROJECT_DIR/requirements.txt
requests
feedparser
EOF

# --- 5. Set Up Python Environment ---
echo "[+] Setting up Python environment..."
python3 -m venv $PROJECT_DIR/venv
$PROJECT_DIR/venv/bin/pip install -r $PROJECT_DIR/requirements.txt

# --- 6. Create Service File ---
echo "[+] Creating systemd service file..."
sudo bash -c "cat << EOF > /etc/systemd/system/business_collector_v2.service
[Unit]
Description=Business Collector Service v2
After=network.target

[Service]
User=$USER
Group=$USER
WorkingDirectory=$PROJECT_DIR
ExecStart=$PROJECT_DIR/venv/bin/python3 business_collector_v2.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF"

# --- 7. Start the Service ---
echo "[+] Starting Business Collector v2 service..."
sudo chown -R $USER:$USER /factory
sudo systemctl daemon-reload
sudo systemctl start business_collector_v2
sudo systemctl enable business_collector_v2

echo "--- Business Collector v2 Setup Complete ---"
echo "To check the status, run: sudo systemctl status business_collector_v2"
echo "To watch the logs, run: tail -f /factory/logs/business_collector_v2.log"
